# -*- coding: utf-8 -*-
"""sat_5114_small2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xd-CO-fvF3GbTbnchFPC9RTwMDb8pCBh
"""

#Import scikit-learn dataset library
import numpy as np
import pandas as pd
from sklearn import svm, datasets
from sklearn.metrics import precision_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
import sklearn.metrics as metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import make_scorer, precision_score, recall_score
#Load dataset
cancer = datasets.load_breast_cancer()

print(cancer.data.shape)
print(cancer.target_names)
# print(cancer.DESCR)
print('cancer dataset features:', cancer.feature_names)

# X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test

#For this small project2 I will be utilising the SVM model itself
 #Here for K-fold cv I will be utilising the 2 kernels of SVM
kernels = ['rbf', 'linear']
C = [1,10,20]
avg_scores = {}
for kval in kernels:
    for cval in C:
        cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),cancer.data, cancer.target, cv=5)
        avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)
        
avg_scores

#For the calculation of precision 
#Dividing the X train and x test first 
# X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109)
y_pred = cross_val_predict(svm.SVC(kernel=kval,C=cval,gamma='auto'),cancer.data, cancer.target, cv=5)
precision = precision_score(cancer.target, y_pred)

print("Precision score: {:.3f}".format(precision))

#For finding the specificity 
tn, fp, fn, tp = confusion_matrix(cancer.target, y_pred).ravel()
specificity = tn / (tn + fp)
print("Specificity: {:.3f}".format(specificity))

"""From the above we can say that linear kernels perform better than the rbf kernel 

"""

#Firstly I will be utilsing the Gridsearch and then let us analyse the randomsearch cv 
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [1, 10, 20], 'kernel': ['rbf', 'linear']}
svm_model = svm.SVC(gamma='auto')
clf = GridSearchCV(svm_model, param_grid, cv=5)
clf.fit(cancer.data, cancer.target)
clf.cv_results_

df = pd.DataFrame(clf.cv_results_)
df

df[['param_C','param_kernel','mean_test_score']]

best_params = clf.best_params_
best_params

param_grid = {'C': [1, 10, 20], 'kernel': ['rbf', 'linear']}
svm_model = svm.SVC(kernel=best_params['kernel'], C=best_params['C'], gamma='auto')
precision_scores = cross_val_score(svm_model, cancer.data, cancer.target, cv=5, scoring='precision')
precision_scores

# Define custom scoring function for specificity
def specificity_score(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)

specificity_scorer = make_scorer(specificity_score)

# Perform grid search to find best SVM parameters
svm_model = svm.SVC(gamma='auto')
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring=specificity_scorer)
grid_search.fit(cancer.data, cancer.target)

# Extract best parameters from grid search results
best_params = grid_search.best_params_

# Train SVM model with best parameters using cross-validation
svm_model = svm.SVC(kernel=best_params['kernel'], C=best_params['C'], gamma='auto')
specificity_scores = cross_val_score(svm_model, cancer.data, cancer.target, cv=5, scoring=specificity_scorer)

# Print mean specificity score
mean_specificity = np.mean(specificity_scores)
print('Mean specificity:', mean_specificity)

# Get predicted labels for test data
predicted_labels = cross_val_predict(svm_model, cancer.data, cancer.target, cv=5)

# Calculate specificity using predicted labels
specificity = specificity_score(cancer.target, predicted_labels)
print('Specificity:', specificity)

# Calculate mean precision and specificity scores across folds
mean_precision = np.mean(precision_scores)
mean_specificity = np.mean(specificity_scores)

# Print mean precision and specificity scores
print('Mean precision:', mean_precision)
print('Mean specificity:', mean_specificity)

clf.best_score_

#Now coming to randomsearch 
from sklearn.model_selection import RandomizedSearchCV
rs = RandomizedSearchCV(svm.SVC(gamma='auto'), {
        'C': [1,10,20],
        'kernel': ['rbf','linear']
    }, 
    cv=5, 
    return_train_score=False, 
    n_iter=2
)
rs.fit(cancer.data, cancer.target)
pd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]

rs.best_params_

model_params = {
    'random_forest': {
        'model': RandomForestClassifier(),
        'params' : {
            'n_estimators': [1,5,10]
        }
    },
    'logistic_regression' : {
        'model': LogisticRegression(solver='liblinear',multi_class='auto'),
        'params': {
            'C': [1,5,10]
        }
    },
    'naive_bayes_gaussian': {
        'model': GaussianNB(),
        'params': {}
    },
    'naive_bayes_multinomial': {
        'model': MultinomialNB(),
        'params': {}
    },
    'decision_tree': {
        'model': DecisionTreeClassifier(),
        'params': {
            'criterion': ['gini','entropy'],
            
        }
    }     
}

from sklearn.model_selection import GridSearchCV
import pandas as pd
scores = []

for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
    clf.fit(cancer.data, cancer.target)
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    
df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df

